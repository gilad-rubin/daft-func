{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56199207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGLE (uniform batched shape): [[RerankedHit(query_uuid='q1', doc_id='d1', score=2.0), RerankedHit(query_uuid='q1', doc_id='d2', score=1.0)]]\n",
      "                                                      d\n",
      "                                                      d\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error when running pipeline node UDF _apply_batch-05e1bacb-3579-49c0-ac0c-cd769e92815a\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 405\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# Multi input (auto → Daft if available, else Python fallback)\u001b[39;00m\n\u001b[32m    395\u001b[39m multi_inputs = {\n\u001b[32m    396\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretriever\u001b[39m\u001b[33m\"\u001b[39m: retriever,\n\u001b[32m    397\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreranker\u001b[39m\u001b[33m\"\u001b[39m: reranker,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m    404\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m multi_out = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMULTI:\u001b[39m\u001b[33m\"\u001b[39m, multi_out[\u001b[33m\"\u001b[39m\u001b[33mreranked_hits\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     batching = (\n\u001b[32m    212\u001b[39m         batching_requested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs[map_axis]) >= \u001b[38;5;28mself\u001b[39m.batch_threshold\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m map_axis\n\u001b[32m    214\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    215\u001b[39m     )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_axis\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m batching \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single(inputs)\n\u001b[32m    219\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 360\u001b[39m, in \u001b[36mRunner._run_batch\u001b[39m\u001b[34m(self, inputs, map_axis)\u001b[39m\n\u001b[32m    357\u001b[39m     df = df.select(*keep_cols, new_col_expr.alias(node.meta.output_name))\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# Finalize: collect wanted outputs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m out_py = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# list of dicts per row, includes all columns\u001b[39;00m\n\u001b[32m    361\u001b[39m merged: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mdict\u001b[39m(constants)\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# normalize final output to List[List[RerankedHit]]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/dataframe/dataframe.py:4279\u001b[39m, in \u001b[36mDataFrame.to_pylist\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4259\u001b[39m \u001b[38;5;129m@DataframePublicAPI\u001b[39m\n\u001b[32m   4260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_pylist\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m   4261\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Converts the current Dataframe into a python list.\u001b[39;00m\n\u001b[32m   4262\u001b[39m \n\u001b[32m   4263\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4277\u001b[39m \u001b[33;03m        [df.iter_rows()][daft.DataFrame.iter_rows]: streaming iterator over individual rows in a DataFrame\u001b[39;00m\n\u001b[32m   4278\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/dataframe/dataframe.py:484\u001b[39m, in \u001b[36mDataFrame.iter_rows\u001b[39m\u001b[34m(self, results_buffer_size, column_format)\u001b[39m\n\u001b[32m    479\u001b[39m partitions_iter = get_or_create_runner().run_iter_tables(\n\u001b[32m    480\u001b[39m     \u001b[38;5;28mself\u001b[39m._builder, results_buffer_size=results_buffer_size\n\u001b[32m    481\u001b[39m )\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Iterate through partitions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpartitions_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn_format\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpython_iter_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/runners/native_runner.py:124\u001b[39m, in \u001b[36mNativeRunner.run_iter_tables\u001b[39m\u001b[34m(self, builder, results_buffer_size)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_iter_tables\u001b[39m(\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m, builder: LogicalPlanBuilder, results_buffer_size: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m ) -> Iterator[MicroPartition]:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_buffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_buffer_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/runners/native_runner.py:115\u001b[39m, in \u001b[36mNativeRunner.run_iter\u001b[39m\u001b[34m(self, builder, results_buffer_size)\u001b[39m\n\u001b[32m    106\u001b[39m results_gen = executor.run(\n\u001b[32m    107\u001b[39m     plan,\n\u001b[32m    108\u001b[39m     {k: v.values() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._part_set_cache.get_all_partition_sets().items()},\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mquery_id\u001b[39m\u001b[33m\"\u001b[39m: query_id},\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults_gen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_notify_result_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/execution/native_executor.py:46\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdaft\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartitioning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LocalMaterializedResult\n\u001b[32m     41\u001b[39m psets_mp = {\n\u001b[32m     42\u001b[39m     part_id: [part.micropartition()._micropartition \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts] \u001b[38;5;28;01mfor\u001b[39;00m part_id, parts \u001b[38;5;129;01min\u001b[39;00m psets.items()\n\u001b[32m     43\u001b[39m }\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mLocalMaterializedResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMicroPartition\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pymicropartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_physical_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpsets_mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_buffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/udf/execution.py:93\u001b[39m, in \u001b[36mcall_batch\u001b[39m\u001b[34m(cls, method, original_args, evaluated_args)\u001b[39m\n\u001b[32m     89\u001b[39m args, kwargs = replace_expressions_with_evaluated_args(original_args, evaluated_args_series)\n\u001b[32m     91\u001b[39m bound_method = \u001b[38;5;28mcls\u001b[39m._daft_bind_method(method)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m output = \u001b[43mbound_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Series):\n\u001b[32m     95\u001b[39m     output_series = output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/udf/udf_v2.py:299\u001b[39m, in \u001b[36mClsBase._daft_bind_method.<locals>.bound_method\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbound_method\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_workspace/dagflow/.venv/lib/python3.12/site-packages/daft/udf/udf_v2.py:84\u001b[39m, in \u001b[36mFunc._from_func.<locals>.method\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmethod\u001b[39m(_self: \u001b[38;5;28;01mNone\u001b[39;00m, *args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 335\u001b[39m, in \u001b[36mRunner._run_batch.<locals>._apply_batch\u001b[39m\u001b[34m(*cols)\u001b[39m\n\u001b[32m    331\u001b[39m     kwargs[name] = constants[name]\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# Don't increment arg_idx for constants\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# This is a dependent column from df\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     raw = \u001b[43mser_lists\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg_idx\u001b[49m\u001b[43m]\u001b[49m[i]\n\u001b[32m    336\u001b[39m     kwargs[name] = deserializers[idx](raw)\n\u001b[32m    337\u001b[39m     arg_idx += \u001b[32m1\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# dagflow_minimal_auto_daft.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple, get_type_hints\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --------------------------\n",
    "# Small domain (schemas + ops)\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def normalize(text: str) -> set[str]:\n",
    "    toks = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "    return {t[:-1] if t.endswith(\"s\") else t for t in toks}\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query_uuid: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "class RetrievalHit(BaseModel):\n",
    "    doc_id: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    query_uuid: str\n",
    "    hits: List[RetrievalHit]\n",
    "\n",
    "\n",
    "class RerankedHit(BaseModel):\n",
    "    query_uuid: str\n",
    "    doc_id: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "class Retriever(Protocol):\n",
    "    def retrieve(self, query: Query, top_k: int) -> RetrievalResult: ...\n",
    "\n",
    "\n",
    "class Reranker(Protocol):\n",
    "    def rerank(\n",
    "        self, query: Query, hits: RetrievalResult, top_k: int\n",
    "    ) -> List[RerankedHit]: ...\n",
    "\n",
    "\n",
    "class ToyRetriever(Retriever):\n",
    "    def __init__(self, corpus: Dict[str, str]):\n",
    "        self._doc_tokens = {doc_id: normalize(txt) for doc_id, txt in corpus.items()}\n",
    "\n",
    "    def retrieve(self, query: Query, top_k: int) -> RetrievalResult:\n",
    "        q = normalize(query.text)\n",
    "        scored = [\n",
    "            RetrievalHit(doc_id=d, score=float(len(q & toks)))\n",
    "            for d, toks in self._doc_tokens.items()\n",
    "        ]\n",
    "        scored.sort(key=lambda h: h.score, reverse=True)\n",
    "        return RetrievalResult(query_uuid=query.query_uuid, hits=scored[:top_k])\n",
    "\n",
    "\n",
    "class IdentityReranker(Reranker):\n",
    "    def rerank(\n",
    "        self, query: Query, hits: RetrievalResult, top_k: int\n",
    "    ) -> List[RerankedHit]:\n",
    "        return [\n",
    "            RerankedHit(query_uuid=query.query_uuid, doc_id=h.doc_id, score=h.score)\n",
    "            for h in hits.hits[:top_k]\n",
    "        ]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Generic DAG decorator + registry\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NodeMeta:\n",
    "    output_name: str\n",
    "    # For auto-batching: which parameter is the per-item \"map axis\"?\n",
    "    map_axis: Optional[str] = None  # e.g., \"query\"\n",
    "    # If nodes align by a key (same item across nodes), what attribute on the map_axis carries it?\n",
    "    key_attr: Optional[str] = None  # e.g., \"query_uuid\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NodeDef:\n",
    "    fn: Callable\n",
    "    meta: NodeMeta\n",
    "    params: Tuple[str, ...]  # ordered parameter names (from signature)\n",
    "\n",
    "\n",
    "class DagRegistry:\n",
    "    def __init__(self):\n",
    "        self.nodes: List[NodeDef] = []\n",
    "        self.by_output: Dict[str, NodeDef] = {}\n",
    "\n",
    "    def add(self, fn: Callable, meta: NodeMeta):\n",
    "        sig = inspect.signature(fn)\n",
    "        params = tuple(sig.parameters.keys())\n",
    "        node = NodeDef(fn=fn, meta=meta, params=params)\n",
    "        self.nodes.append(node)\n",
    "        self.by_output[meta.output_name] = node\n",
    "\n",
    "    def topo(self, initial_inputs: Dict[str, Any]) -> List[NodeDef]:\n",
    "        \"\"\"Very small topo-sort by parameter availability.\"\"\"\n",
    "        available = set(initial_inputs.keys())\n",
    "        ordered: List[NodeDef] = []\n",
    "        remaining = set(self.nodes)\n",
    "        while remaining:\n",
    "            progress = False\n",
    "            for node in list(remaining):\n",
    "                needed = {p for p in node.params if p not in (\"self\",)}\n",
    "                if needed.issubset(available):\n",
    "                    ordered.append(node)\n",
    "                    available.add(node.meta.output_name)\n",
    "                    remaining.remove(node)\n",
    "                    progress = True\n",
    "            if not progress:\n",
    "                raise RuntimeError(\n",
    "                    f\"Cannot resolve dependencies; remaining: {[n.fn.__name__ for n in remaining]}\"\n",
    "                )\n",
    "        return ordered\n",
    "\n",
    "\n",
    "_REG = DagRegistry()\n",
    "\n",
    "\n",
    "def dagflow(\n",
    "    *, output: str, map_axis: Optional[str] = None, key_attr: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Register a DAG node.\n",
    "    - output: name of the produced value (binds it into the DAG namespace)\n",
    "    - map_axis: name of the parameter that carries the per-item object (for multi-input runs)\n",
    "    - key_attr: attribute on the map_axis object that uniquely identifies items (alignment)\n",
    "    \"\"\"\n",
    "    meta = NodeMeta(output_name=output, map_axis=map_axis, key_attr=key_attr)\n",
    "\n",
    "    def deco(fn: Callable):\n",
    "        @wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return fn(*args, **kwargs)\n",
    "\n",
    "        wrapper._dagflow_meta = meta\n",
    "        _REG.add(wrapper, meta)\n",
    "        return wrapper\n",
    "\n",
    "    return deco\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# DAG nodes (generic; tiny)\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "@dagflow(output=\"hits\", map_axis=\"query\", key_attr=\"query_uuid\")\n",
    "def retrieve(retriever: Retriever, query: Query, top_k: int) -> RetrievalResult:\n",
    "    return retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "\n",
    "@dagflow(output=\"reranked_hits\", map_axis=\"query\", key_attr=\"query_uuid\")\n",
    "def rerank(\n",
    "    reranker: Reranker, query: Query, hits: RetrievalResult, top_k: int\n",
    ") -> List[RerankedHit]:\n",
    "    return reranker.rerank(query, hits, top_k=top_k)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Generic Runner\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "class Runner:\n",
    "    \"\"\"\n",
    "    - Single item: executes locally (pure Python), returns [[RerankedHit]] for uniformity.\n",
    "    - Multi items: automatically uses Daft for vectorized execution (falls back to Python if Daft not available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode: str = \"auto\", batch_threshold: int = 2):\n",
    "        self.mode = mode\n",
    "        self.batch_threshold = batch_threshold\n",
    "\n",
    "    def run(self, *, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the DAG given initial inputs.\n",
    "        The only special key is the map_axis object (e.g., \"query\"); it may be a single item or a list of items.\n",
    "        \"\"\"\n",
    "        # Determine if we are batching based on any node's map_axis param presence + list input\n",
    "        map_axes = {n.meta.map_axis for n in _REG.nodes if n.meta.map_axis}\n",
    "        if len(map_axes) > 1:\n",
    "            raise ValueError(\n",
    "                f\"This minimal runner supports one map axis; found: {map_axes}\"\n",
    "            )\n",
    "        map_axis = next(iter(map_axes)) if map_axes else None\n",
    "\n",
    "        batching_requested = False\n",
    "        if map_axis and map_axis in inputs and isinstance(inputs[map_axis], list):\n",
    "            batching_requested = True\n",
    "\n",
    "        if self.mode == \"local\":\n",
    "            batching = False\n",
    "        elif self.mode == \"daft\":\n",
    "            batching = True\n",
    "        else:\n",
    "            batching = (\n",
    "                batching_requested and len(inputs[map_axis]) >= self.batch_threshold\n",
    "                if map_axis\n",
    "                else False\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            self._run_batch(inputs, map_axis) if batching else self._run_single(inputs)\n",
    "        )\n",
    "\n",
    "    # ---- single item path (pure Python) ----\n",
    "    def _run_single(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        outputs = dict(inputs)\n",
    "        order = _REG.topo(inputs)\n",
    "        for node in order:\n",
    "            kwargs = {p: outputs[p] for p in node.params if p in outputs}\n",
    "            res = node.fn(**kwargs)\n",
    "            outputs[node.meta.output_name] = res\n",
    "        # normalize to batched shape if final output is present\n",
    "        if \"reranked_hits\" in outputs and isinstance(outputs[\"reranked_hits\"], list):\n",
    "            outputs[\"reranked_hits\"] = [outputs[\"reranked_hits\"]]\n",
    "        return outputs\n",
    "\n",
    "    # ---- multi item path (Daft if available; else Python loop) ----\n",
    "    def _run_batch(\n",
    "        self, inputs: Dict[str, Any], map_axis: Optional[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        # Safety: require a single map axis for this minimal demo\n",
    "        assert map_axis, \"No map axis configured but batching was requested.\"\n",
    "\n",
    "        items: List[BaseModel] = inputs[map_axis]  # e.g., List[Query]\n",
    "        constants = {k: v for k, v in inputs.items() if k != map_axis}\n",
    "\n",
    "        try:\n",
    "            import daft\n",
    "        except Exception:\n",
    "            # Fallback to Python loop using the same single-run logic per item\n",
    "            aggregated: List[Dict[str, Any]] = []\n",
    "            for it in items:\n",
    "                per_inputs = {**constants, map_axis: it}\n",
    "                per_out = self._run_single(per_inputs)\n",
    "                aggregated.append(per_out)\n",
    "            # Merge structure: final outputs to lists\n",
    "            merged: Dict[str, Any] = dict(constants)\n",
    "            # For final output example, ensure list-of-lists\n",
    "            merged[\"reranked_hits\"] = [o[\"reranked_hits\"][0] for o in aggregated]\n",
    "            return merged\n",
    "\n",
    "        # Build initial Daft DF with one column for the map_axis (dictified Pydantic)\n",
    "        df = daft.from_pylist([{map_axis: it.model_dump()} for it in items])\n",
    "\n",
    "        order = _REG.topo(\n",
    "            {**constants, map_axis: items[0]}\n",
    "        )  # topo over a \"sample\" shape\n",
    "\n",
    "        # We'll iteratively add columns to df for each node's output.\n",
    "        for node in order:\n",
    "            # Collect columns for this node call: mapped param -> series; constants -> literals; deps -> series\n",
    "            arg_names = node.params\n",
    "\n",
    "            # Build a list of expressions to feed into the batch UDF in the right order\n",
    "            series_args: List[Any] = []\n",
    "            serializers: List[Callable[[Any], Dict]] = []\n",
    "            deserializers: List[Callable[[Dict], Any]] = []\n",
    "\n",
    "            # Figure out types from hints (for Pydantic reconstruction)\n",
    "            hints = get_type_hints(node.fn)\n",
    "\n",
    "            def _mk_deser(py_type):\n",
    "                # Minimal: if subclass of BaseModel, rehydrate; else, passthrough\n",
    "                try:\n",
    "                    if isinstance(py_type, type) and issubclass(py_type, BaseModel):\n",
    "                        return lambda d: py_type.model_validate(d)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                return lambda x: x\n",
    "\n",
    "            for name in arg_names:\n",
    "                if name == node.meta.map_axis:\n",
    "                    series_args.append(df[map_axis])\n",
    "                    deserializers.append(_mk_deser(hints.get(name, Any)))\n",
    "                    serializers.append(\n",
    "                        lambda x: x.model_dump() if isinstance(x, BaseModel) else x\n",
    "                    )\n",
    "                elif name in constants:\n",
    "                    # constants captured via closure (not as series)\n",
    "                    series_args.append(None)\n",
    "                    deserializers.append(lambda x: x)  # not used\n",
    "                    serializers.append(lambda x: x)  # not used\n",
    "                elif name in df.column_names:\n",
    "                    series_args.append(df[name])\n",
    "                    deserializers.append(_mk_deser(hints.get(name, Any)))\n",
    "                    serializers.append(\n",
    "                        lambda x: x.model_dump() if isinstance(x, BaseModel) else x\n",
    "                    )\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Parameter '{name}' not found among inputs/columns for node {node.fn.__name__}\"\n",
    "                    )\n",
    "\n",
    "            # Define a batch UDF that rebuilds per-row kwargs, calls node.fn, returns per-row result (dict)\n",
    "            @daft.func.batch(return_dtype=daft.DataType.python())\n",
    "            def _apply_batch(*cols):\n",
    "                # cols only includes series (constants were filtered out)\n",
    "                # Convert Series to pylist per column (Daft passes daft.Series)\n",
    "                ser_lists: List[List[Any]] = []\n",
    "                for c in cols:\n",
    "                    ser_lists.append(c.to_pylist())\n",
    "\n",
    "                out_list: List[Any] = []\n",
    "                rows = len(ser_lists[0]) if ser_lists else len(items)\n",
    "                for i in range(rows):\n",
    "                    kwargs: Dict[str, Any] = {}\n",
    "                    arg_idx = 0\n",
    "                    for idx, name in enumerate(arg_names):\n",
    "                        if name == node.meta.map_axis:\n",
    "                            raw = ser_lists[arg_idx][i]\n",
    "                            kwargs[name] = deserializers[idx](raw)\n",
    "                            arg_idx += 1\n",
    "                        elif name in constants:\n",
    "                            kwargs[name] = constants[name]\n",
    "                            # Don't increment arg_idx for constants\n",
    "                        else:\n",
    "                            # This is a dependent column from df\n",
    "                            raw = ser_lists[arg_idx][i]\n",
    "                            kwargs[name] = deserializers[idx](raw)\n",
    "                            arg_idx += 1\n",
    "                    # call the original node function\n",
    "                    res = node.fn(**kwargs)\n",
    "                    # store as dict (Pydantic -> dict; list[Pydantic] -> list[dict])\n",
    "                    if isinstance(res, BaseModel):\n",
    "                        out_list.append(res.model_dump())\n",
    "                    elif (\n",
    "                        isinstance(res, list) and res and isinstance(res[0], BaseModel)\n",
    "                    ):\n",
    "                        out_list.append([r.model_dump() for r in res])\n",
    "                    else:\n",
    "                        out_list.append(res)\n",
    "                return out_list\n",
    "\n",
    "            # Build a new DataFrame with the new column appended\n",
    "            # We must pass only the Series columns to _apply_batch\n",
    "            call_series = [c for c in series_args if c is not None]\n",
    "            new_col_expr = _apply_batch(*call_series)\n",
    "            # Keep existing columns + add the new one\n",
    "            keep_cols = [df[c].alias(c) for c in df.column_names]\n",
    "            df = df.select(*keep_cols, new_col_expr.alias(node.meta.output_name))\n",
    "\n",
    "        # Finalize: collect wanted outputs\n",
    "        out_py = df.to_pylist()  # list of dicts per row, includes all columns\n",
    "        merged: Dict[str, Any] = dict(constants)\n",
    "        # normalize final output to List[List[RerankedHit]]\n",
    "        final_name = order[-1].meta.output_name\n",
    "        merged[final_name] = [\n",
    "            [RerankedHit.model_validate(d) for d in row[final_name]] for row in out_py\n",
    "        ]\n",
    "        return merged\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Demo\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = {\n",
    "        \"d1\": \"a quick brown fox jumps\",\n",
    "        \"d2\": \"brown dog sleeps\",\n",
    "        \"d3\": \"five boxing wizards jump quickly\",\n",
    "    }\n",
    "    retriever = ToyRetriever(corpus)\n",
    "    reranker = IdentityReranker()\n",
    "    runner = Runner(mode=\"auto\", batch_threshold=2)\n",
    "\n",
    "    # Single input (local)\n",
    "    single_inputs = {\n",
    "        \"retriever\": retriever,\n",
    "        \"reranker\": reranker,\n",
    "        \"query\": Query(query_uuid=\"q1\", text=\"quick brown\"),\n",
    "        \"top_k\": 2,  # shared by both nodes\n",
    "    }\n",
    "    single_out = runner.run(inputs=single_inputs)\n",
    "    print(\"SINGLE (uniform batched shape):\", single_out[\"reranked_hits\"])\n",
    "\n",
    "    # Multi input (auto → Daft if available, else Python fallback)\n",
    "    multi_inputs = {\n",
    "        \"retriever\": retriever,\n",
    "        \"reranker\": reranker,\n",
    "        \"query\": [\n",
    "            Query(query_uuid=\"q1\", text=\"quick brown\"),\n",
    "            Query(query_uuid=\"q2\", text=\"wizards jump\"),\n",
    "            Query(query_uuid=\"q3\", text=\"brown dog\"),\n",
    "        ],\n",
    "        \"top_k\": 2,\n",
    "    }\n",
    "    multi_out = runner.run(inputs=multi_inputs)\n",
    "    print(\"MULTI:\", multi_out[\"reranked_hits\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43403251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: retrieve\n",
      "  Output: hits\n",
      "  Map axis: query\n",
      "  Params: ('retriever', 'query', 'top_k')\n",
      "\n",
      "Node: rerank\n",
      "  Output: reranked_hits\n",
      "  Map axis: query\n",
      "  Params: ('reranker', 'query', 'hits', 'top_k')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's trace the issue\n",
    "corpus = {\n",
    "    \"d1\": \"a quick brown fox jumps\",\n",
    "    \"d2\": \"brown dog sleeps\",\n",
    "    \"d3\": \"five boxing wizards jump quickly\",\n",
    "}\n",
    "retriever = ToyRetriever(corpus)\n",
    "reranker = IdentityReranker()\n",
    "\n",
    "# Check what parameters each node has\n",
    "for node in _REG.nodes:\n",
    "    print(f\"Node: {node.fn.__name__}\")\n",
    "    print(f\"  Output: {node.meta.output_name}\")\n",
    "    print(f\"  Map axis: {node.meta.map_axis}\")\n",
    "    print(f\"  Params: {node.params}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7defba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial df columns: ['query']\n",
      "\n",
      "Node rerank params: ('reranker', 'query', 'hits', 'top_k')\n",
      "Map axis: query\n",
      "\n",
      "0. reranker -> constant (NO SERIES)\n",
      "1. query -> map_axis (SERIES)\n",
      "2. hits -> dependent column (SERIES)\n",
      "3. top_k -> constant (NO SERIES)\n",
      "\n",
      "So call_series should have 2 items: [query_series, hits_series]\n",
      "And when iterating with enumerate:\n",
      "  idx=0, name='reranker' -> constant, don't use arg_idx\n",
      "  idx=1, name='query' -> map_axis, use arg_idx=0, then increment to 1\n",
      "  idx=2, name='hits' -> dependent, use arg_idx=1, then increment to 2\n",
      "  idx=3, name='top_k' -> constant, don't use arg_idx\n",
      "\n",
      "But arg_idx=2 is out of range for a 2-item list!\n"
     ]
    }
   ],
   "source": [
    "# More detailed debugging - let's manually trace what should happen\n",
    "import daft\n",
    "\n",
    "# Simulate first node (retrieve)\n",
    "items_test = [\n",
    "    Query(query_uuid=\"q1\", text=\"quick brown\"),\n",
    "    Query(query_uuid=\"q2\", text=\"wizards jump\"),\n",
    "]\n",
    "df_test = daft.from_pylist([{\"query\": it.model_dump()} for it in items_test])\n",
    "print(\"Initial df columns:\", df_test.column_names)\n",
    "print()\n",
    "\n",
    "# After first node, we'd have query + hits columns\n",
    "# Let's check indexing for second node (rerank)\n",
    "node = _REG.by_output[\"reranked_hits\"]\n",
    "constants_test = {\"retriever\": retriever, \"reranker\": reranker, \"top_k\": 2}\n",
    "\n",
    "print(f\"Node {node.fn.__name__} params: {node.params}\")\n",
    "print(f\"Map axis: {node.meta.map_axis}\")\n",
    "print()\n",
    "\n",
    "# Simulate what series_args would look like\n",
    "for idx, name in enumerate(node.params):\n",
    "    if name == node.meta.map_axis:\n",
    "        print(f\"{idx}. {name} -> map_axis (SERIES)\")\n",
    "    elif name in constants_test:\n",
    "        print(f\"{idx}. {name} -> constant (NO SERIES)\")\n",
    "    else:\n",
    "        print(f\"{idx}. {name} -> dependent column (SERIES)\")\n",
    "\n",
    "print()\n",
    "print(\"So call_series should have 2 items: [query_series, hits_series]\")\n",
    "print(\"And when iterating with enumerate:\")\n",
    "print(\"  idx=0, name='reranker' -> constant, don't use arg_idx\")\n",
    "print(\"  idx=1, name='query' -> map_axis, use arg_idx=0, then increment to 1\")\n",
    "print(\"  idx=2, name='hits' -> dependent, use arg_idx=1, then increment to 2\")\n",
    "print(\"  idx=3, name='top_k' -> constant, don't use arg_idx\")\n",
    "print()\n",
    "print(\"But arg_idx=2 is out of range for a 2-item list!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e814be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dagflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
